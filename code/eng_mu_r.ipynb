{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3Ty8WKqTD7c4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/nagendra/system_files/bin/english/english_clean_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wgu0hYCdEBNy",
    "outputId": "f8832e46-59f2-474f-ed86-f17ed5b8fc3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 5383\n",
      "Testing data size: 951\n"
     ]
    }
   ],
   "source": [
    "# prompt: train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming 'df' is your pandas DataFrame with 'cleaned' and 'is_hateful' columns\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)  # Adjust test_size and random_state as needed\n",
    "\n",
    "print(f\"Training data size: {len(train_df)}\")\n",
    "print(f\"Testing data size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8GDWWuA5D6Uw",
    "outputId": "7fb0e5d6-b5b4-44c3-87bd-90d114f4feb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 5383\n"
     ]
    }
   ],
   "source": [
    "train_df['cleaned'] = train_df['cleaned'].astype(str).str.strip()  # Ensure all entries are strings and strip whitespace\n",
    "print(f\"Training data size: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8OMcAvNTzpm",
    "outputId": "15f2c9de-93d0-4fa1-f96e-7d742d35d7e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 951\n"
     ]
    }
   ],
   "source": [
    "test_df['cleaned'] = test_df['cleaned'].astype(str).str.strip()  # Ensure all entries are strings and strip whitespace\n",
    "print(f\"Training data size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nKCqjGdvYXju"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get text data, ensure it's a string\n",
    "        text = self.data.iloc[idx][\"cleaned\"]\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\"\n",
    "            \n",
    "        # Get the label\n",
    "        label = self.data.iloc[idx][\"is_hateful\"]\n",
    "\n",
    "        # Safely evaluate the 'rational_3' column to get the attention vector\n",
    "        try:\n",
    "            attention_vector = eval(self.data.iloc[idx][\"rational_1\"])\n",
    "            if not isinstance(attention_vector, list):\n",
    "                attention_vector = []\n",
    "        except (SyntaxError, NameError):\n",
    "            attention_vector = []\n",
    "\n",
    "        # Tokenize the text\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        # Convert attention_vector to a tensor and pad if necessary\n",
    "        attention_vector = torch.tensor(attention_vector, dtype=torch.float)\n",
    "\n",
    "        \n",
    "#          # Safely evaluate the 'rational_3' column to get the attention vector\n",
    "#         try:\n",
    "#             attention_vector = eval(self.data.iloc[idx][\"rational_2\"])\n",
    "#             if not isinstance(attention_vector, list):\n",
    "#                 attention_vector = []\n",
    "#         except (SyntaxError, NameError):\n",
    "#             attention_vector = []\n",
    "        \n",
    " # Tokenize the text\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        # Adjust ground truth attention to match subword tokens\n",
    "        adjusted_ground_truth_attention = self.adjust_ground_truth_attention(text, tokens, attention_vector)\n",
    "\n",
    "        # Convert to torch tensor and pad to match max_len\n",
    "        attention_vector = torch.tensor(adjusted_ground_truth_attention, dtype=torch.float)\n",
    "\n",
    "        padding_length = self.max_len - attention_vector.size(0)\n",
    "\n",
    "        if padding_length > 0:\n",
    "            # Pad the attention vector with zeros to match max_len\n",
    "            attention_vector = torch.cat([attention_vector, torch.zeros(padding_length)], dim=0)\n",
    "        else:\n",
    "            # Truncate the attention vector if it's longer than max_len\n",
    "            attention_vector = attention_vector[:self.max_len]\n",
    "\n",
    "        # Tokenized inputs\n",
    "        input_ids = inputs['input_ids'].squeeze(0)  # Remove batch dimension\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        token_type_ids = inputs['token_type_ids'].squeeze(0)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'label': label,\n",
    "            'attention_vector': attention_vector  # No unsqueeze(0) here\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def adjust_ground_truth_attention(self, sentence, tokens, ground_truth_attention):\n",
    "        \"\"\"\n",
    "        Adjusts ground truth attention to match the subword tokens generated by the BERT tokenizer.\n",
    "        If a word is split into subwords, the attention for the original word is repeated for each subword.\n",
    "        \"\"\"\n",
    "        adjusted_attention = []\n",
    "        word_idx = 0  # Index for words in sentence.split()\n",
    "\n",
    "        for token in tokens:\n",
    "            # If token starts with '##', it's a subword, so repeat the last attention value\n",
    "            if token.startswith(\"##\"):\n",
    "                adjusted_attention.append(adjusted_attention[-1])\n",
    "            else:\n",
    "                # For new words, append the original ground truth attention value\n",
    "                if word_idx < len(ground_truth_attention):\n",
    "                    adjusted_attention.append(ground_truth_attention[word_idx])\n",
    "                    word_idx += 1\n",
    "                else:\n",
    "                    # Handle cases where the number of tokens exceeds the attention values\n",
    "                    adjusted_attention.append(0)\n",
    "\n",
    "        return adjusted_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "afj5yt4yYviP"
   },
   "outputs": [],
   "source": [
    "class BertWithAttentionSupervision(nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(BertWithAttentionSupervision, self).__init__()\n",
    "        self.bert_classifier = BertForSequenceClassification.from_pretrained(\"google/muril-base-cased\", num_labels=num_labels)\n",
    "        self.attention_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels, ground_truth_attention=None):\n",
    "        outputs = self.bert_classifier(input_ids=input_ids,\n",
    "                                       attention_mask=attention_mask,\n",
    "                                       token_type_ids=token_type_ids,\n",
    "                                       labels=labels,\n",
    "                                       output_attentions=True,\n",
    "                                       return_dict=True)\n",
    "\n",
    "        classification_loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Extract attention scores from the final layer\n",
    "        attention_scores = outputs.attentions[-1][:, :, 0, :]  # CLS token attention across all heads\n",
    "        avg_attention_scores = attention_scores.mean(dim=1)  # Average attention scores across heads\n",
    "\n",
    "        # Calculate attention loss only if ground_truth_attention is provided\n",
    "        attention_loss = 0\n",
    "        if ground_truth_attention is not None:\n",
    "            if ground_truth_attention.size(0) != avg_attention_scores.size(0):\n",
    "                raise ValueError(f\"Expected ground_truth_attention to be of size {avg_attention_scores.size(0)}, but got {ground_truth_attention.size(0)}\")\n",
    "            attention_loss = self.attention_loss_fn(avg_attention_scores, ground_truth_attention)\n",
    "\n",
    "        return logits, classification_loss, attention_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3sQ8DKnY0q4",
    "outputId": "4623fe31-97f9-4dac-ffb1-1c3ca6a8ae86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Total Loss: 0.6748\n",
      "0.7802313354363828\n",
      "Model updated\n",
      "Epoch 2/100, Total Loss: 0.5823\n",
      "0.8349106203995794\n",
      "Model updated\n",
      "Epoch 3/100, Total Loss: 0.5254\n",
      "0.8370136698212408\n",
      "Model updated\n",
      "Epoch 4/100, Total Loss: 0.4860\n",
      "0.8412197686645636\n",
      "Model updated\n",
      "Epoch 5/100, Total Loss: 0.4463\n",
      "Epoch 6/100, Total Loss: 0.4345\n",
      "Epoch 7/100, Total Loss: 0.4103\n",
      "Epoch 8/100, Total Loss: 0.3980\n",
      "Epoch 9/100, Total Loss: 0.3950\n",
      "Epoch 10/100, Total Loss: 0.3906\n",
      "Epoch 11/100, Total Loss: 0.3832\n",
      "Epoch 12/100, Total Loss: 0.3785\n",
      "Epoch 13/100, Total Loss: 0.3721\n",
      "Epoch 14/100, Total Loss: 0.3743\n",
      "Epoch 15/100, Total Loss: 0.3717\n",
      "Epoch 16/100, Total Loss: 0.3672\n",
      "Epoch 17/100, Total Loss: 0.3624\n",
      "Epoch 18/100, Total Loss: 0.3570\n",
      "Epoch 19/100, Total Loss: 0.3536\n",
      "Epoch 20/100, Total Loss: 0.3514\n",
      "Epoch 21/100, Total Loss: 0.3508\n",
      "Epoch 22/100, Total Loss: 0.3507\n",
      "Epoch 23/100, Total Loss: 0.3635\n",
      "Epoch 24/100, Total Loss: 0.3666\n",
      "Epoch 25/100, Total Loss: 0.3599\n",
      "Epoch 26/100, Total Loss: 0.3510\n",
      "Epoch 27/100, Total Loss: 0.3508\n",
      "Epoch 28/100, Total Loss: 0.3499\n",
      "Epoch 29/100, Total Loss: 0.3492\n",
      "Epoch 30/100, Total Loss: 0.3490\n",
      "Epoch 31/100, Total Loss: 0.3489\n",
      "Epoch 32/100, Total Loss: 0.3487\n",
      "Epoch 33/100, Total Loss: 0.3485\n",
      "Epoch 34/100, Total Loss: 0.3485\n",
      "Epoch 35/100, Total Loss: 0.3485\n",
      "Epoch 36/100, Total Loss: 0.3484\n",
      "Epoch 37/100, Total Loss: 0.3484\n",
      "Epoch 38/100, Total Loss: 0.3483\n",
      "Epoch 39/100, Total Loss: 0.3485\n",
      "Epoch 40/100, Total Loss: 0.3752\n",
      "Epoch 41/100, Total Loss: 0.3533\n",
      "Epoch 42/100, Total Loss: 0.3501\n",
      "Epoch 43/100, Total Loss: 0.3502\n",
      "Epoch 44/100, Total Loss: 0.3520\n",
      "Epoch 45/100, Total Loss: 0.3492\n",
      "Epoch 46/100, Total Loss: 0.3483\n",
      "Epoch 47/100, Total Loss: 0.3483\n",
      "Epoch 48/100, Total Loss: 0.3483\n",
      "Epoch 49/100, Total Loss: 0.3482\n",
      "Epoch 50/100, Total Loss: 0.3482\n",
      "Epoch 51/100, Total Loss: 0.3482\n",
      "Epoch 52/100, Total Loss: 0.3482\n",
      "Epoch 53/100, Total Loss: 0.3482\n",
      "Epoch 54/100, Total Loss: 0.3482\n",
      "Epoch 55/100, Total Loss: 0.3482\n",
      "Epoch 56/100, Total Loss: 0.3482\n",
      "Epoch 57/100, Total Loss: 0.3481\n",
      "Epoch 58/100, Total Loss: 0.3487\n",
      "Epoch 59/100, Total Loss: 0.3489\n",
      "Epoch 60/100, Total Loss: 0.3489\n",
      "Epoch 61/100, Total Loss: 0.3544\n",
      "Epoch 62/100, Total Loss: 0.3662\n",
      "Epoch 63/100, Total Loss: 0.3506\n",
      "Epoch 64/100, Total Loss: 0.3500\n",
      "Epoch 65/100, Total Loss: 0.3483\n",
      "Epoch 66/100, Total Loss: 0.3482\n",
      "Epoch 67/100, Total Loss: 0.3481\n",
      "Epoch 68/100, Total Loss: 0.3481\n",
      "Epoch 69/100, Total Loss: 0.3481\n",
      "Epoch 70/100, Total Loss: 0.3481\n",
      "Epoch 71/100, Total Loss: 0.3481\n",
      "Epoch 72/100, Total Loss: 0.3481\n",
      "Epoch 73/100, Total Loss: 0.3481\n",
      "Epoch 74/100, Total Loss: 0.3481\n",
      "Epoch 75/100, Total Loss: 0.3481\n",
      "Epoch 76/100, Total Loss: 0.3481\n",
      "Epoch 77/100, Total Loss: 0.3481\n",
      "Epoch 78/100, Total Loss: 0.3481\n",
      "Epoch 79/100, Total Loss: 0.3481\n",
      "Epoch 80/100, Total Loss: 0.3690\n",
      "Epoch 81/100, Total Loss: 0.3499\n",
      "Epoch 82/100, Total Loss: 0.3484\n",
      "Epoch 83/100, Total Loss: 0.3482\n",
      "Epoch 84/100, Total Loss: 0.3481\n",
      "Epoch 85/100, Total Loss: 0.3480\n",
      "Epoch 86/100, Total Loss: 0.3481\n",
      "Epoch 87/100, Total Loss: 0.3480\n",
      "Epoch 88/100, Total Loss: 0.3480\n",
      "Epoch 89/100, Total Loss: 0.3480\n",
      "Epoch 90/100, Total Loss: 0.3480\n",
      "Epoch 91/100, Total Loss: 0.3480\n",
      "Epoch 92/100, Total Loss: 0.3480\n",
      "Epoch 93/100, Total Loss: 0.3480\n",
      "Epoch 94/100, Total Loss: 0.3480\n",
      "Epoch 95/100, Total Loss: 0.3480\n",
      "Epoch 96/100, Total Loss: 0.3480\n",
      "Epoch 97/100, Total Loss: 0.3490\n",
      "Epoch 98/100, Total Loss: 0.3622\n",
      "Epoch 99/100, Total Loss: 0.3520\n",
      "Epoch 100/100, Total Loss: 0.3496\n"
     ]
    }
   ],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, train_dataloader, eval_dataloader, optimizer, device, alph=0.7, bet=0.3, epochs=30):\n",
    "    model.train()\n",
    "    \n",
    "    acc1 = 0\n",
    "    acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move data to device (GPU or CPU)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            ground_truth_attention = batch['attention_vector'].to(device)\n",
    "\n",
    "            try:\n",
    "                # Forward pass\n",
    "                logits, classification_loss, attention_loss = model(input_ids=input_ids,\n",
    "                                                                attention_mask=attention_mask,\n",
    "                                                                token_type_ids=token_type_ids,\n",
    "                                                                labels=labels,\n",
    "                                                                ground_truth_attention=ground_truth_attention)\n",
    "\n",
    "                # Total loss with weights\n",
    "                total_batch_loss = alph * classification_loss + bet * attention_loss\n",
    "\n",
    "                # Backpropagation\n",
    "                total_batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Accumulate total loss\n",
    "                total_loss += total_batch_loss.item()\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch: {batch}\")\n",
    "                raise e\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Total Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Evaluate the model after each epoch\n",
    "        acc1 = evaluate_model(model, eval_dataloader, device)\n",
    "        if acc1 > acc:\n",
    "                acc = acc1\n",
    "                print(acc1)\n",
    "            \n",
    "            # Save the trained model\n",
    "                torch.save(model.state_dict(), \"/home/nagendra/system_files/bin/english/with_rationales/murill/muril_attention_model_new1.pth\")\n",
    "                print(\"Model updated\")\n",
    "            \n",
    "            \n",
    "# Evaluate model\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            logits, _, _ = model(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  token_type_ids=token_type_ids,\n",
    "                                  labels=None,\n",
    "                                  ground_truth_attention=None)  # Set to None for evaluation\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "#     precision = precision_score(all_labels, all_predictions, average='binary')\n",
    "#     recall = recall_score(all_labels, all_predictions, average='binary')\n",
    "#     f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "\n",
    "#     macro_precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "#     macro_recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "#     macro_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    return accuracy\n",
    "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
    "#     print(f\"Precision: {precision:.4f}\")\n",
    "#     print(f\"Recall: {recall:.4f}\")\n",
    "#     print(f\"F1 Score: {f1:.4f}\")\n",
    "#     print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "#     print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "#     print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "    max_len = 128  # Adjust based on your dataset\n",
    "#     hindi_tokens = train_df['clean_text'].dropna().str.split().explode().unique().tolist()\n",
    "\n",
    "#     vocab = tokenizer.get_vocab()\n",
    "#     new_tokens = set(hindi_tokens) - set(vocab.keys())\n",
    "\n",
    "#     if new_tokens:\n",
    "#         tokenizer.add_tokens(list(new_tokens))\n",
    "    \n",
    "#     tokenizer.save_pretrained(\"/home/nagendra/system_files/bin/Hindi/with_rationales/muril/tokenizer\")\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = CustomTextDataset(train_df, tokenizer, max_len=max_len)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "    eval_dataset = CustomTextDataset(test_df, tokenizer, max_len=max_len)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BertWithAttentionSupervision(num_labels=2).to(device)\n",
    "\n",
    "    # Resize embeddings after adding new tokens\n",
    "    model.bert_classifier.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_dataloader, eval_dataloader, optimizer, device, alph=0.5, bet=0.5, epochs=100)\n",
    "\n",
    "\n",
    "def evaluate_loaded_model(device):\n",
    "    # Load the tokenizer first\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/home/nagendra/system_files/bin/english/with_rationales/murill/tokenizer\")\n",
    "    \n",
    "    # Load the saved model for evaluation\n",
    "    loaded_model = BertWithAttentionSupervision(num_labels=2).to(device)\n",
    "    \n",
    "#     # Ensure you resize the embeddings after loading the tokenizer\n",
    "#     loaded_model.bert_classifier.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    loaded_model.load_state_dict(torch.load(\"/home/nagendra/system_files/bin/english/with_rationales/murill/muril_attention_model_new1.pth\"))\n",
    "    print(\"Model loaded for evaluation.\")\n",
    "\n",
    "    # Create a separate evaluation dataset and dataloader\n",
    "    eval_dataset = CustomTextDataset(test_df, tokenizer, max_len=128)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Evaluate the loaded model\n",
    "    evaluate_model(loaded_model, eval_dataloader, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     evaluate_loaded_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/zia/shashi/english/with_rationales/muril/output_large.txt', 'w') as file:\n",
    "#     file.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4145168576.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1080787/4145168576.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Model loaded for evaluation.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# muril_attention_model_new.pth (0.4, 0.6, 30) 2e-5, 64\n",
    "\n",
    "Model loaded for evaluation.\n",
    "Evaluation Metrics:\n",
    "Accuracy: 0.8465\n",
    "Precision: 0.8789\n",
    "Recall: 0.8699\n",
    "F1 Score: 0.8744\n",
    "Macro Precision: 0.8376\n",
    "Macro Recall: 0.8396\n",
    "Macro F1 Score: 0.8385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muril_attention_model_large.pth (0.7, 0.3, 30)\n",
    "\n",
    "Model loaded for evaluation.\n",
    "Evaluation Metrics:\n",
    "Accuracy: 0.8496\n",
    "Precision: 0.8681\n",
    "Recall: 0.8904\n",
    "F1 Score: 0.8791\n",
    "Macro Precision: 0.8431\n",
    "Macro Recall: 0.8376\n",
    "Macro F1 Score: 0.8401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muril_attention_model2.pth (0.7, 0.3, 30)\n",
    "\n",
    "Model loaded for evaluation.\n",
    "Evaluation Metrics:\n",
    "Accuracy: 0.8307\n",
    "Precision: 0.8259\n",
    "Recall: 0.9178\n",
    "F1 Score: 0.8694\n",
    "Macro Precision: 0.8335\n",
    "Macro Recall: 0.8050\n",
    "Macro F1 Score: 0.8144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muril_attention_model3.pth (0.75, 0.25, 30)\n",
    "\n",
    "Model loaded for evaluation.\n",
    "Evaluation Metrics:\n",
    "Accuracy: 0.8286\n",
    "Precision: 0.8636\n",
    "Recall: 0.8562\n",
    "F1 Score: 0.8598\n",
    "Macro Precision: 0.8189\n",
    "Macro Recall: 0.8205\n",
    "Macro F1 Score: 0.8196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wT4e1ydQp6fV"
   },
   "outputs": [],
   "source": [
    "# class BertWithAttentionSupervision(nn.Module):\n",
    "#     def __init__(self, num_labels=2):\n",
    "#         super(BertWithAttentionSupervision, self).__init__()\n",
    "#         self.bert_classifier = BertForSequenceClassification.from_pretrained(\"google/muril-base-cased\", num_labels=num_labels)\n",
    "#         self.attention_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask, token_type_ids, labels, ground_truth_attention=None):\n",
    "#         outputs = self.bert_classifier(input_ids=input_ids,\n",
    "#                                        attention_mask=attention_mask,\n",
    "#                                        token_type_ids=token_type_ids,\n",
    "#                                        labels=labels,\n",
    "#                                        output_attentions=True,\n",
    "#                                        return_dict=True)\n",
    "\n",
    "#         classification_loss = outputs.loss\n",
    "#         logits = outputs.logits\n",
    "\n",
    "#         # Extract attention scores from the final layer\n",
    "#         attention_scores = outputs.attentions[-1][:, :, 0, :]  # CLS token attention across all heads\n",
    "#         avg_attention_scores = attention_scores.mean(dim=1)  # Average attention scores across heads\n",
    "\n",
    "#         # Calculate attention loss only if ground_truth_attention is provided\n",
    "#         attention_loss = 0\n",
    "#         if ground_truth_attention is not None:\n",
    "#             if ground_truth_attention.size(0) != avg_attention_scores.size(0):\n",
    "#                 raise ValueError(f\"Expected ground_truth_attention to be of size {avg_attention_scores.size(0)}, but got {ground_truth_attention.size(0)}\")\n",
    "#             attention_loss = self.attention_loss_fn(avg_attention_scores, ground_truth_attention)\n",
    "\n",
    "#         return logits, classification_loss, attention_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            logits, _, _ = model(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  token_type_ids=token_type_ids,\n",
    "                                  labels=None,\n",
    "                                  ground_truth_attention=None)  # Set to None for evaluation\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='binary')\n",
    "    recall = recall_score(all_labels, all_predictions, average='binary')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    macro_precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "    macro_recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "    macro_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "    print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_loaded_model(device):\n",
    "    # Load the tokenizer first\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "    \n",
    "    # Load the saved model for evaluation\n",
    "    loaded_model = BertWithAttentionSupervision(num_labels=2).to(device)\n",
    "    \n",
    "    # Ensure you resize the embeddings after loading the tokenizer\n",
    "    loaded_model.bert_classifier.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    loaded_model.load_state_dict(torch.load(\"/home/nagendra/system_files/bin/english/with_rationales/murill/muril_attention_model_new.pth\"))\n",
    "    print(\"Model loaded for evaluation.\")\n",
    "\n",
    "    # Create a separate evaluation dataset and dataloader\n",
    "    eval_dataset = CustomTextDataset(test_df, tokenizer, max_len=128)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Evaluate the loaded model\n",
    "    evaluate_model(loaded_model, eval_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1400526/2443557719.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(\"/home/nagendra/system_files/bin/english/with_rationales/murill/muril_attention_model_new.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.8465\n",
      "Precision: 0.8789\n",
      "Recall: 0.8699\n",
      "F1 Score: 0.8744\n",
      "Macro Precision: 0.8376\n",
      "Macro Recall: 0.8396\n",
      "Macro F1 Score: 0.8385\n"
     ]
    }
   ],
   "source": [
    "# %%capture cap\n",
    "evaluate_loaded_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HBD2avQbacT",
    "outputId": "bf6bfb6b-95f9-4797-d82d-7a0054fee42f"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# loaded_model = BertWithAttentionSupervision(num_labels=2).to(device)\n",
    "# loaded_model.load_state_dict(torch.load(\"/home/zia/shashi/english/with_rationales/muril/muril_attention_model.pth\",map_location=device))\n",
    "# loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371,
     "referenced_widgets": [
      "4fea4282464b49ffb160dc6ca2e1e926",
      "e7945fe70bf84b62a1c13fdb2de0b0f4",
      "7515a849a8a740af8931707ac453c2bb",
      "3633f65a53d54458838942bf5ef82bf3",
      "f0f2b465328d4dada3e6f534aac8fb2e",
      "1b3b8fd19f734b6f9983b57f69569728",
      "01c05184a14f4392a58c9e92d70d315e",
      "7cbc89cb81734c3bbca37a233fca95e4",
      "5548b83ad02c4baaa91c8e7d448afcf3",
      "0dfde3e733e947088b4fd766f2094c4e",
      "a80a0c9cfc2a47bc97d587d4bec33f82",
      "d9f3ba55354646dba335685050af7984",
      "9c80c33aa9664cddab3aa48f2b3405b9",
      "3b53f458595047a8af63a96f27575145",
      "ee9ade3cef7b46108a7052409faaf4b4",
      "c6e5c1d023d54ed2b70ab75b0dbcd352",
      "b55801d33a134d52be5520d143df91d4",
      "fa0b2d93961c478aa3f9a72c547a6bae",
      "6ddd5a72809c4a34b73272ada22d94c4",
      "78da83e7ccac44acb5adf323a9257f3a",
      "8641202df0e440b3a7d971c33916c171",
      "4872d422e6e14489a59a6de4704e6a77",
      "e75588faa02b4c058909f7a0bd5195d2",
      "37765ac4e0574af6ab10b628c08f4b07",
      "b3c1855c36a143839fe85dd1a8c2fbc1",
      "331320e713044b8d9e647d8312482848",
      "fe22e46f87a34b46b77445b7803e5f1b",
      "f74f3df8c5f34b30abbdeedc85328e83",
      "2831effda3df43e2b2bbffa6a617322e",
      "981d58d0a3c1482b8d466e3b7052b6c0",
      "45baaba63f804972a8ec674339e3e403",
      "65ffa26830534ee6b818e544a2872a25",
      "f87ebf90346f46318580c7cfbe8dc140"
     ]
    },
    "id": "wHtYCg2_b-iO",
    "outputId": "cdf6dc4f-8f50-4aa3-c816-53a6cb1e967f"
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "# max_len = 128\n",
    "# eval_dataset = CustomTextDataset(test_df, tokenizer, max_len=max_len)\n",
    "# eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# # Evaluate the loaded model\n",
    "# evaluate_model(loaded_model, eval_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j2GIgUBgm1LH",
    "outputId": "0edcfc29-6c5e-40b5-b54e-8d3b5b7befcc"
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "# def get_attention_scores(model, tokenizer, text):\n",
    "#     # Tokenize input text\n",
    "#     inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "#     # input_ids = inputs['input_ids']\n",
    "\n",
    "#     # Get the attention weights by running a forward pass\n",
    "#     with torch.no_grad():\n",
    "#         # Forward pass through the model with output_attentions=True to get attention scores\n",
    "#         outputs = model(**inputs, labels=None)\n",
    "#         print(nn.Softmax(outputs[0]))\n",
    "\n",
    "#     # Extract attentions from model outputs\n",
    "#     attentions = outputs[3]  # This is a tuple of attention scores from different layers\n",
    "\n",
    "#     # Return tokenized input and attention scores from the last layer\n",
    "#     return inputs, attentions[-1]  # Taking the last layer's attention weights\n",
    "\n",
    "# def explain_token_attention(tokenizer, inputs, attention_scores):\n",
    "#     input_ids = inputs['input_ids'][0]  # First sequence in batch\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "#     # Average the attention scores across heads for simplicity\n",
    "#     attention_mean = attention_scores.mean(dim=1)  # Average over all heads\n",
    "\n",
    "#     # Get attention weights for each token\n",
    "#     explanations = {}\n",
    "#     for i, token in enumerate(tokens):\n",
    "#         token_attention = attention_mean[:, i].mean().item()  # Average over source tokens\n",
    "#         explanations[token] = token_attention\n",
    "\n",
    "#     return explanations\n",
    "\n",
    "# # Example usage\n",
    "# text = \"you are such a bitch\"\n",
    "# inputs, attention_scores = get_attention_scores(loaded_model, tokenizer, text)\n",
    "# explanations = explain_token_attention(tokenizer, inputs, attention_scores)\n",
    "\n",
    "# # Print token-wise explanations\n",
    "# for token, score in explanations.items():\n",
    "#     print(f\"Token: {token}, Attention: {score}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01c05184a14f4392a58c9e92d70d315e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0dfde3e733e947088b4fd766f2094c4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b3b8fd19f734b6f9983b57f69569728": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2831effda3df43e2b2bbffa6a617322e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "331320e713044b8d9e647d8312482848": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65ffa26830534ee6b818e544a2872a25",
      "placeholder": "​",
      "style": "IPY_MODEL_f87ebf90346f46318580c7cfbe8dc140",
      "value": " 466k/466k [00:00&lt;00:00, 2.44MB/s]"
     }
    },
    "3633f65a53d54458838942bf5ef82bf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0dfde3e733e947088b4fd766f2094c4e",
      "placeholder": "​",
      "style": "IPY_MODEL_a80a0c9cfc2a47bc97d587d4bec33f82",
      "value": " 48.0/48.0 [00:00&lt;00:00, 3.35kB/s]"
     }
    },
    "37765ac4e0574af6ab10b628c08f4b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f74f3df8c5f34b30abbdeedc85328e83",
      "placeholder": "​",
      "style": "IPY_MODEL_2831effda3df43e2b2bbffa6a617322e",
      "value": "tokenizer.json: 100%"
     }
    },
    "3b53f458595047a8af63a96f27575145": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ddd5a72809c4a34b73272ada22d94c4",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_78da83e7ccac44acb5adf323a9257f3a",
      "value": 231508
     }
    },
    "45baaba63f804972a8ec674339e3e403": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4872d422e6e14489a59a6de4704e6a77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4fea4282464b49ffb160dc6ca2e1e926": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e7945fe70bf84b62a1c13fdb2de0b0f4",
       "IPY_MODEL_7515a849a8a740af8931707ac453c2bb",
       "IPY_MODEL_3633f65a53d54458838942bf5ef82bf3"
      ],
      "layout": "IPY_MODEL_f0f2b465328d4dada3e6f534aac8fb2e"
     }
    },
    "5548b83ad02c4baaa91c8e7d448afcf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "65ffa26830534ee6b818e544a2872a25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ddd5a72809c4a34b73272ada22d94c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7515a849a8a740af8931707ac453c2bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cbc89cb81734c3bbca37a233fca95e4",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5548b83ad02c4baaa91c8e7d448afcf3",
      "value": 48
     }
    },
    "78da83e7ccac44acb5adf323a9257f3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7cbc89cb81734c3bbca37a233fca95e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8641202df0e440b3a7d971c33916c171": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "981d58d0a3c1482b8d466e3b7052b6c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c80c33aa9664cddab3aa48f2b3405b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b55801d33a134d52be5520d143df91d4",
      "placeholder": "​",
      "style": "IPY_MODEL_fa0b2d93961c478aa3f9a72c547a6bae",
      "value": "vocab.txt: 100%"
     }
    },
    "a80a0c9cfc2a47bc97d587d4bec33f82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3c1855c36a143839fe85dd1a8c2fbc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_981d58d0a3c1482b8d466e3b7052b6c0",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45baaba63f804972a8ec674339e3e403",
      "value": 466062
     }
    },
    "b55801d33a134d52be5520d143df91d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6e5c1d023d54ed2b70ab75b0dbcd352": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9f3ba55354646dba335685050af7984": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9c80c33aa9664cddab3aa48f2b3405b9",
       "IPY_MODEL_3b53f458595047a8af63a96f27575145",
       "IPY_MODEL_ee9ade3cef7b46108a7052409faaf4b4"
      ],
      "layout": "IPY_MODEL_c6e5c1d023d54ed2b70ab75b0dbcd352"
     }
    },
    "e75588faa02b4c058909f7a0bd5195d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_37765ac4e0574af6ab10b628c08f4b07",
       "IPY_MODEL_b3c1855c36a143839fe85dd1a8c2fbc1",
       "IPY_MODEL_331320e713044b8d9e647d8312482848"
      ],
      "layout": "IPY_MODEL_fe22e46f87a34b46b77445b7803e5f1b"
     }
    },
    "e7945fe70bf84b62a1c13fdb2de0b0f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b3b8fd19f734b6f9983b57f69569728",
      "placeholder": "​",
      "style": "IPY_MODEL_01c05184a14f4392a58c9e92d70d315e",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "ee9ade3cef7b46108a7052409faaf4b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8641202df0e440b3a7d971c33916c171",
      "placeholder": "​",
      "style": "IPY_MODEL_4872d422e6e14489a59a6de4704e6a77",
      "value": " 232k/232k [00:00&lt;00:00, 1.76MB/s]"
     }
    },
    "f0f2b465328d4dada3e6f534aac8fb2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f74f3df8c5f34b30abbdeedc85328e83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f87ebf90346f46318580c7cfbe8dc140": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa0b2d93961c478aa3f9a72c547a6bae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe22e46f87a34b46b77445b7803e5f1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
