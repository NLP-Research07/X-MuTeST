{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6ac929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8067f7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lime in /home/nagendra/anaconda3/lib/python3.9/site-packages (0.2.0.1)\n",
      "Requirement already satisfied: numpy in /home/nagendra/anaconda3/lib/python3.9/site-packages (from lime) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from lime) (1.0.2)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from lime) (0.19.2)\n",
      "Requirement already satisfied: matplotlib in /home/nagendra/anaconda3/lib/python3.9/site-packages (from lime) (3.5.2)\n",
      "Requirement already satisfied: tqdm in /home/nagendra/anaconda3/lib/python3.9/site-packages (from lime) (4.64.1)\n",
      "Requirement already satisfied: scipy in /home/nagendra/anaconda3/lib/python3.9/site-packages (from lime) (1.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.12->lime) (21.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.12->lime) (1.3.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.12->lime) (2.19.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.12->lime) (9.2.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.12->lime) (2.8.4)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.12->lime) (2021.7.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.18->lime) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.18->lime) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from matplotlib->lime) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from matplotlib->lime) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from matplotlib->lime) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/nagendra/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed747b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: safetensors in /home/nagendra/anaconda3/lib/python3.9/site-packages (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca39e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/nagendra/anaconda3/lib/python3.9/site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0916341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 02:25:55.433824: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-30 02:25:55.433948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-30 02:25:55.435421: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-30 02:25:55.444230: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-30 02:25:56.540501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "import torch\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification , BertTokenizer , BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4266d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithAttentionSupervision(nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(BertWithAttentionSupervision, self).__init__()\n",
    "        self.bert_classifier = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=num_labels)\n",
    "        self.attention_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, ground_truth_attention=None):\n",
    "        outputs = self.bert_classifier(input_ids=input_ids,\n",
    "                                       attention_mask=attention_mask,\n",
    "                                       #token_type_ids=token_type_ids,\n",
    "                                       labels=labels,\n",
    "                                       output_attentions=True,\n",
    "                                       return_dict=True)\n",
    "\n",
    "        classification_loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Extract attention scores from the final layer\n",
    "        attention_scores = outputs.attentions[-1][:, :, 0, :]  # CLS token attention across all heads\n",
    "        avg_attention_scores = attention_scores.mean(dim=1)  # Average attention scores across heads\n",
    "\n",
    "        # Calculate attention loss only if ground_truth_attention is provided\n",
    "        attention_loss = 0\n",
    "        if ground_truth_attention is not None:\n",
    "            if ground_truth_attention.size(0) != avg_attention_scores.size(0):\n",
    "                raise ValueError(f\"Expected ground_truth_attention to be of size {avg_attention_scores.size(0)}, but got {ground_truth_attention.size(0)}\")\n",
    "            attention_loss = self.attention_loss_fn(avg_attention_scores, ground_truth_attention)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3140521",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2483f9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_4075755/4124861956.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(\"/home/nagendra/system_files/bin/english/with_rationales/xlmrl/xlmr_attention_model1.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loaded_model = BertWithAttentionSupervision(num_labels=2).to(device)\n",
    "\n",
    "# Load the model state dict with map_location for CPU\n",
    "loaded_model.load_state_dict(torch.load(\"/home/nagendra/system_files/bin/english/with_rationales/xlmrl/xlmr_attention_model1.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a97bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class names\n",
    "class_names = ['non-hate', 'hate']\n",
    "\n",
    "# Predictor function for LIME\n",
    "def predictor(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = loaded_model(**inputs)\n",
    "    probas = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "    return probas\n",
    "\n",
    "# Initialize LIME Text Explainer\n",
    "explainer = LimeTextExplainer(class_names=class_names, split_expression='\\s+')\n",
    "\n",
    "data = pd.read_csv('/home/nagendra/system_files/bin/english/with_rationales/english_clean_new.csv')\n",
    "\n",
    "texts = data['cleaned'].astype(str).tolist()\n",
    "labels = data['is_hateful'].tolist()\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.15, random_state=42  # stratify to maintain label distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1559ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_excel = []\n",
    "\n",
    "# Loop through each text in test_texts\n",
    "for i, text in enumerate(test_texts):\n",
    "    # Explain the instance for the current text\n",
    "    exp = explainer.explain_instance(text, predictor, num_features=200, num_samples=500)\n",
    "\n",
    "    # Get the explanation map and list\n",
    "    explanation_map = exp.as_map()\n",
    "    explanation_list = exp.as_list()\n",
    "\n",
    "    # Concatenate the lists based on matching y values\n",
    "    combined_explanation_list = [\n",
    "        (x, y, z) for (x, y) in explanation_list for (z, y2) in explanation_map[1] if y == y2\n",
    "    ]\n",
    "\n",
    "    # Sort the combined list based on the third element (z)\n",
    "    sorted_combined_explanation_list = sorted(combined_explanation_list, key=lambda item: item[2])\n",
    "\n",
    "    # Extract tokens and exact_hateful values\n",
    "    tokens = [item[0] for item in sorted_combined_explanation_list]\n",
    "    exact_hateful = [item[1] for item in sorted_combined_explanation_list]\n",
    "    is_hateful = [1 if item[1] > 0 else 0 for item in sorted_combined_explanation_list]\n",
    "\n",
    "    # Add the data to the list for Excel export\n",
    "    data_for_excel.append({\n",
    "        'full_text': text,    # Full text of the current instance\n",
    "        'tokens': tokens,     # List of tokens\n",
    "        'is_hateful': is_hateful,  # List of is_hateful values\n",
    "        'exact_hateful': exact_hateful\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_xcel = pd.DataFrame(data_for_excel)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df_xcel.to_excel(\"english_xlmr_rational.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6c13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
